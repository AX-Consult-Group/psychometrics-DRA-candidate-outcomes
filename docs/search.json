[
  {
    "objectID": "psychometrics-candidate-outcomes.html",
    "href": "psychometrics-candidate-outcomes.html",
    "title": "Psychometric Risk Assessment: Employment & Placement Outcomes",
    "section": "",
    "text": "This report presents an example analysis of how psychometric assessment scores from the Dynamic Risk Assessment (DRA) relate to real-world employment and placement outcomes."
  },
  {
    "objectID": "psychometrics-candidate-outcomes.html#descriptive-statistics",
    "href": "psychometrics-candidate-outcomes.html#descriptive-statistics",
    "title": "Psychometric Risk Assessment: Employment & Placement Outcomes",
    "section": "Descriptive Statistics",
    "text": "Descriptive Statistics\nThis section provides an overview of the simulated data, including correlations among DRA dimensions and risk scores, outlier detection, and group comparisons across key outcomes.\n\nCorrelations Among DRA Dimensions & Risk Scores\nThe Spearman correlations show strong interrelationships among the DRA dimensions and risk components:\n\nRisk Mitigators shows a strong, negative association with Risk Drivers (-0.77, p&lt;0.001), as expected in the assessment design.\nSimilarly, the Composite Risk Score shows a strong, positive correlation with Risk Drivers (0.92, p&lt;0.001) and a strong negative correlation with Risk Mitigators (-0.95, p&lt;0.001).\n\nWeak or non-significant correlations (e.g., Uncertainty with Principles, r=0.00) highlight independent aspects of the profile.\n\n\nShow code\ncorrelations &lt;- raw_data %&gt;% \n  dplyr::select(c(1:8))\n\nflattenCorrMatrix &lt;- function(cormat, pmat) {\n  ut &lt;- upper.tri(cormat)\n  data.frame(\n    row=rownames(cormat)[row(cormat)[ut]],\n    column=rownames(cormat)[col(cormat)[ut]],\n    cor=(cormat)[ut],\n    p=pmat[ut]\n  )\n}\n\ncor_sig &lt;- Hmisc::rcorr(as.matrix(correlations), type = \"spearman\") \n\ncorrelation_table &lt;- flattenCorrMatrix(cor_sig$r, cor_sig$P) %&gt;%\n  mutate(\n    cor = round(cor, 3),   \n    p   = round(p, 3)       \n  ) \n\ncorrelation_table %&gt;%\n  mutate(\n    cor = round(cor, 3),\n    p   = round(p, 3),\n    ` ` = case_when(p &lt; 0.001 ~ \"***\", p &lt; 0.01 ~ \"**\", p &lt; 0.05 ~ \"*\", TRUE ~ \"\")\n  ) %&gt;%\n  select(`Variable 1` = row, `Variable 2` = column, Correlation = cor, `p-value` = p, Significance = ` `) %&gt;%\n  kable(\n    caption = \"Spearman Correlations (significant p &lt; 0.05)\",\n    digits  = 3,\n    align   = c(\"l\", \"l\", \"r\", \"r\", \"c\")\n  )\n\n\n\nSpearman Correlations (significant p &lt; 0.05)\n\n\n\n\n\n\n\n\n\nVariable 1\nVariable 2\nCorrelation\np-value\nSignificance\n\n\n\n\nCore_Traits\nEmotional_Understanding\n-0.230\n0.001\n**\n\n\nCore_Traits\nJudgement\n0.262\n0.000\n***\n\n\nEmotional_Understanding\nJudgement\n-0.018\n0.792\n\n\n\nCore_Traits\nPrinciples\n0.171\n0.011\n*\n\n\nEmotional_Understanding\nPrinciples\n0.093\n0.172\n\n\n\nJudgement\nPrinciples\n0.152\n0.025\n*\n\n\nCore_Traits\nUncertainty\n0.044\n0.521\n\n\n\nEmotional_Understanding\nUncertainty\n-0.098\n0.149\n\n\n\nJudgement\nUncertainty\n0.443\n0.000\n***\n\n\nPrinciples\nUncertainty\n0.003\n0.960\n\n\n\nCore_Traits\nRisk_Mitigators\n-0.683\n0.000\n***\n\n\nEmotional_Understanding\nRisk_Mitigators\n-0.183\n0.007\n**\n\n\nJudgement\nRisk_Mitigators\n-0.380\n0.000\n***\n\n\nPrinciples\nRisk_Mitigators\n-0.502\n0.000\n***\n\n\nUncertainty\nRisk_Mitigators\n-0.085\n0.211\n\n\n\nCore_Traits\nRisk_Drivers\n0.593\n0.000\n***\n\n\nEmotional_Understanding\nRisk_Drivers\n0.329\n0.000\n***\n\n\nJudgement\nRisk_Drivers\n0.353\n0.000\n***\n\n\nPrinciples\nRisk_Drivers\n0.539\n0.000\n***\n\n\nUncertainty\nRisk_Drivers\n-0.025\n0.719\n\n\n\nRisk_Mitigators\nRisk_Drivers\n-0.766\n0.000\n***\n\n\nCore_Traits\nComposite_Risk_Score\n0.683\n0.000\n***\n\n\nEmotional_Understanding\nComposite_Risk_Score\n0.255\n0.000\n***\n\n\nJudgement\nComposite_Risk_Score\n0.396\n0.000\n***\n\n\nPrinciples\nComposite_Risk_Score\n0.544\n0.000\n***\n\n\nUncertainty\nComposite_Risk_Score\n0.029\n0.673\n\n\n\nRisk_Mitigators\nComposite_Risk_Score\n-0.952\n0.000\n***\n\n\nRisk_Drivers\nComposite_Risk_Score\n0.915\n0.000\n***\n\n\n\n\n\n\n\nShow code\nmodel.matrix(~0+., correlations) %&gt;% \n  cor(use='pairwise.complete.obs', method = \"spearman\") %&gt;% \n  ggcorrplot(sig.level=0.5, \n             hc.order = TRUE, \n             show.diag = FALSE, \n             type = \"lower\", \n             lab_size=0.5) +\n  xlab(\"Variable 1\") + \n  ylab(\"Variable 2\") +\n  theme_apa(legend.pos = \"right\", legend.use.title = FALSE,\n  legend.font.size = 12, x.font.size = 12, y.font.size = 12,\n  facet.title.size = 12, remove.y.gridlines = TRUE,\n  remove.x.gridlines = TRUE) +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))\n\n\n\n\n\n\n\nChecking for Outliers\n\n\nShow code\n# Detecting outliers using the IQR method \n\nQ1 &lt;- quantile(raw_data$Composite_Risk_Score, 0.25)\nQ3 &lt;- quantile(raw_data$Composite_Risk_Score, 0.75)\nIQR &lt;- Q3 - Q1\noutliers &lt;- which(raw_data$Composite_Risk_Score&lt; (Q1 - 1.5 * IQR) | raw_data$Composite_Risk_Score &gt; (Q3 + 1.5 * IQR))\n\nif (length(outliers) &gt; 0) {\n  # Extract the values of outliers\n  outlier_values &lt;- raw_data$Composite_Risk_Score[outliers]\n\n  # Print values\n  print(raw_data)\n} else {\n  print(\"No outliers were found using the IQR method.\")\n}\n\n\n[1] \"No outliers were found using the IQR method.\"\n\n\nNo outliers were detected in the Composite Risk Score using the IQR method, suggesting the data is free from extreme values that could skew results.\n\n\nWork Readiness Completion\n\n\nShow code\nraw_data %&gt;%\n  dplyr::select(Work_Readiness_Completion) %&gt;%\n  na.omit() %&gt;%\n  group_by(Work_Readiness_Completion) %&gt;%\n  summarise(Count = n()) %&gt;%\n  ungroup() %&gt;%                    \n  mutate(\n    Percentage = round(Count / sum(Count) * 100, 1),\n    Percentage = paste0(Percentage, \"%\")\n  ) %&gt;%\n  kable(\n    caption = \"Work Readiness Completion\",\n    align = c(\"l\", \"r\", \"r\")\n  )\n\n\n\nWork Readiness Completion\n\n\nWork_Readiness_Completion\nCount\nPercentage\n\n\n\n\nYes\n217\n100%\n\n\n\n\n\nSince all 217 candidates completed the work readiness assessment, this variable was excluded for predictive analysis.\n\n\nPlacement Contract Completion\n\n\nShow code\nraw_data %&gt;%\n  dplyr::select(Placement_Contract_Completion) %&gt;%\n  na.omit() %&gt;%\n  group_by(Placement_Contract_Completion) %&gt;%\n  summarise(Count = n()) %&gt;%\n  ungroup() %&gt;%                    \n  mutate(\n    Percentage = round(Count / sum(Count) * 100, 1),\n    Percentage = paste0(Percentage, \"%\")\n  ) %&gt;%\n  kable(\n    caption = \"Placement Contract Completion\",\n    align = c(\"l\", \"r\", \"r\")\n  )\n\n\n\nPlacement Contract Completion\n\n\nPlacement_Contract_Completion\nCount\nPercentage\n\n\n\n\nIn progress\n51\n23.5%\n\n\nNo\n24\n11.1%\n\n\nYes\n142\n65.4%\n\n\n\n\n\nThe majority (65.7%) completed their placement contract, with 23.1% still in progress and 11.1% not completing.\n\n\nShow code\nPlacement_Contract_Completion_Means &lt;- raw_data %&gt;%\n  dplyr::select(c(Placement_Contract_Completion, Composite_Risk_Score)) %&gt;% \n  na.omit() %&gt;% \n  group_by(Placement_Contract_Completion) %&gt;%\n  summarise(Mean_Composite_Risk = mean(Composite_Risk_Score)) %&gt;% \n  arrange(Mean_Composite_Risk) %&gt;%\n  dplyr::mutate(Mean_Composite_Risk = round(Mean_Composite_Risk, 2))\n\nkable(Placement_Contract_Completion_Means)\n\n\n\n\n\nPlacement_Contract_Completion\nMean_Composite_Risk\n\n\n\n\nIn progress\n31.49\n\n\nYes\n34.05\n\n\nNo\n36.06\n\n\n\n\n\nEmployees who are still completing their placement contract had the lowest average composite risk score (31.49) whereas employees who did not complete their placement contracts had the highest average composite risk score (36.06).\n\n\nShow code\nggplot(raw_data,\n       aes(x = Placement_Contract_Completion,\n           y = Composite_Risk_Score,\n           fill = Placement_Contract_Completion)) +\n   geom_boxplot(outlier.shape = NA, \n                width = 0.55,\n                color = \"grey\") +\n  labs(\n    title = \"Composite Risk Score Distribution: Placement Contract Completion\",\n    subtitle = \"Higher scores indicate greater overall risk\",\n    x = \"Placement Contract Completion\",\n    y = \"Composite Risk Score\"\n  ) +\n  theme_minimal(base_size = 14) +\n  theme(\n    legend.position = \"none\",\n    plot.title = element_text(\n      hjust = 0.5, \n      face = \"bold\",\n      size = 16,\n    ),\n    plot.subtitle = element_text(\n      hjust = 0.5, \n      color = \"grey50\", \n      size = 12\n    ),\n    axis.title = element_text(),\n    axis.text.x = element_text(angle = 45, vjust = 0.5, size = 12),  \n    axis.text.y = element_text(size = 11),\n    panel.grid.major = element_blank(),      # remove major grid lines\n    panel.grid.minor = element_blank(),      # remove minor grid lines\n    panel.border = element_rect(colour = \"grey80\", fill = NA, linewidth = 0.5),\n  ) +\n  \n   scale_fill_manual(values = c(\n    \"No\"  = \"#6A1B9A\",     \n    \"Yes\" = \"#9A4EAE\",\n    \"In progress\" = \"#F3E5F5\"\n  )) +\n  \n  \n  # Slight y-axis expansion for better spacing\n  scale_y_continuous(expand = expansion(mult = c(0.05, 0.1)))\n\n\n\n\n\n\n\nSpecialized Training Completion\n\n\nShow code\nraw_data %&gt;%\n  dplyr::select(Specialised_Training_Completion) %&gt;%\n  na.omit() %&gt;%\n  group_by(Specialised_Training_Completion) %&gt;%\n  summarise(Count = n()) %&gt;%\n  ungroup() %&gt;%                    \n  mutate(\n    Percentage = round(Count / sum(Count) * 100, 1),\n    Percentage = paste0(Percentage, \"%\")\n  ) %&gt;%\n  kable(\n    caption = \"Specialised Training Completion\",\n    align = c(\"l\", \"r\", \"r\")\n  )\n\n\n\nSpecialised Training Completion\n\n\nSpecialised_Training_Completion\nCount\nPercentage\n\n\n\n\nNo\n36\n16.6%\n\n\nYes\n181\n83.4%\n\n\n\n\n\nMost candidates (83.4%) completed specialised training.\n\n\nShow code\nSpecialised_Training_Completion_Means &lt;- raw_data %&gt;%\n  dplyr::select(c(Specialised_Training_Completion, Composite_Risk_Score)) %&gt;% \n  na.omit() %&gt;% \n  group_by(Specialised_Training_Completion) %&gt;%\n  summarise(Mean_Composite_Risk = mean(Composite_Risk_Score)) %&gt;% \n  arrange(Mean_Composite_Risk) %&gt;%\n  dplyr::mutate(Mean_Composite_Risk = round(Mean_Composite_Risk, 2))\n\nkable(Specialised_Training_Completion_Means)\n\n\n\n\n\nSpecialised_Training_Completion\nMean_Composite_Risk\n\n\n\n\nYes\n32.45\n\n\nNo\n39.81\n\n\n\n\n\nEmployees who completed specialized training had a lower average composite risk score compared to those who did not.\n\n\nShow code\nggplot(raw_data,\n       aes(x = Specialised_Training_Completion,\n           y = Composite_Risk_Score,\n           fill = Specialised_Training_Completion)) +\n   geom_boxplot(outlier.shape = NA, \n                width = 0.55,\n                color = \"grey\") +\n  labs(\n    title = \"Composite Risk Score Distribution: Specialised Training Completion\",\n    subtitle = \"Higher scores indicate greater overall risk\",\n    x = \"Specialised Training Completion\",\n    y = \"Composite Risk Score\"\n  ) +\n  theme_minimal(base_size = 14) +\n  theme(\n    legend.position = \"none\",\n    plot.title = element_text(\n      hjust = 0.5, \n      face = \"bold\",\n      size = 16,\n    ),\n    plot.subtitle = element_text(\n      hjust = 0.5, \n      color = \"grey50\", \n      size = 12\n    ),\n    axis.title = element_text(),\n    axis.text.x = element_text(angle = 0, vjust = 0.5, size = 12),  # no angle needed for two categories\n    axis.text.y = element_text(size = 11),\n    panel.grid.major = element_blank(),      # remove major grid lines\n    panel.grid.minor = element_blank(),      # remove minor grid lines\n    panel.border = element_rect(colour = \"grey80\", fill = NA, linewidth = 0.5),\n  ) +\n  \n scale_fill_manual(values = c(\n    \"No\"  = \"#6A1B9A\",     \n    \"Yes\" = \"#9A4EAE\"     \n  )) +\n  \n  # Slight y-axis expansion for better spacing\n  scale_y_continuous(expand = expansion(mult = c(0.05, 0.1)))\n\n\n\n\n\n\n\nCurrent Employment Status\n\n\nShow code\nraw_data %&gt;%\n  dplyr::select(Current_Employment_Status) %&gt;%\n  na.omit() %&gt;%\n  group_by(Current_Employment_Status) %&gt;%\n  summarise(Count = n()) %&gt;%\n  ungroup() %&gt;%                    \n  mutate(\n    Percentage = round(Count / sum(Count) * 100, 1),\n    Percentage = paste0(Percentage, \"%\")\n  ) %&gt;%\n  kable(\n    caption = \"Current Employment Status\",\n    align = c(\"l\", \"r\", \"r\")\n  )\n\n\n\nCurrent Employment Status\n\n\nCurrent_Employment_Status\nCount\nPercentage\n\n\n\n\nFull_time\n26\n12%\n\n\nPart_time\n16\n7.4%\n\n\nSelf_employed\n1\n0.5%\n\n\nTemporary_Contract\n118\n54.4%\n\n\nUnemployed\n56\n25.8%\n\n\n\n\n\nThe most common employment status is Temporary Contract (54.4%), followed by Unemployed (25.8%). Self employed was excluded due to rarity (n=1) resulting in a sample size of 216.\n\n\nShow code\nraw_data &lt;- raw_data %&gt;% \n  group_by(Current_Employment_Status) %&gt;%\n  filter(Current_Employment_Status != \"Self_employed\") %&gt;% \n  ungroup()\n  \nCurrent_Employment_Status_Means &lt;- raw_data %&gt;%\n  ungroup() %&gt;% \n  dplyr::select(c(Current_Employment_Status, Composite_Risk_Score)) %&gt;% \n  na.omit() %&gt;% \n  group_by(Current_Employment_Status) %&gt;%\n  filter(Current_Employment_Status != \"Self_employed\") %&gt;% \n  summarise(Mean_Composite_Risk = mean(Composite_Risk_Score)) %&gt;% \n  arrange(Mean_Composite_Risk) %&gt;%\n  dplyr::mutate(Mean_Composite_Risk = round(Mean_Composite_Risk, 2))\n\nkable(Current_Employment_Status_Means)\n\n\n\n\n\nCurrent_Employment_Status\nMean_Composite_Risk\n\n\n\n\nUnemployed\n28.59\n\n\nTemporary_Contract\n34.59\n\n\nFull_time\n37.94\n\n\nPart_time\n38.52\n\n\n\n\n\nEmployees with part-time contracts have the highest average composite risk (38.52), while candidates who are currently unemployed have the lowest (28.59).\n\n\nShow code\nggplot(raw_data,\n       aes(x = Current_Employment_Status,\n           y = Composite_Risk_Score,\n           fill = Current_Employment_Status)) +\n   geom_boxplot(outlier.shape = NA, \n                width = 0.55,\n                color = \"grey\") +\n  labs(\n    title = \"Composite Risk Score Distribution: Current Employment Status\",\n    subtitle = \"Higher scores indicate greater overall risk\",\n    x = \"Current Employment Status\",\n    y = \"Composite Risk Score\"\n  ) +\n  theme_minimal(base_size = 14) +\n  theme(\n    legend.position = \"none\",\n    plot.title = element_text(\n      hjust = 0.5, \n      face = \"bold\",\n      size = 16,\n    ),\n    plot.subtitle = element_text(\n      hjust = 0.5, \n      color = \"grey50\", \n      size = 12\n    ),\n    axis.title = element_text(),\n    axis.text.x = element_text(angle = 45, vjust = 0.5, size = 12),  \n    axis.text.y = element_text(size = 11),\n    panel.grid.major = element_blank(),      # remove major grid lines\n    panel.grid.minor = element_blank(),      # remove minor grid lines\n    panel.border = element_rect(colour = \"grey80\", fill = NA, linewidth = 0.5),\n  ) +\n  \n scale_fill_manual(values = c(\n    \"Unemployed\"  = \"#6A1B9A\", \"Full_time\" = \"#9A4EAE\", \"Part_time\" = \"#F3E5F5\",\"Temporary_Contract\" = \"#7E57C2\")) +\n  \n  # Slight y-axis expansion for better spacing\n  scale_y_continuous(expand = expansion(mult = c(0.05, 0.1)))\n\n\n\n\n\n\n\nAbsorbed by Host Company\n\n\nShow code\nraw_data %&gt;%\n  dplyr::select(Absorbed_Host_Company) %&gt;%\n  na.omit() %&gt;%\n  group_by(Absorbed_Host_Company) %&gt;%\n  summarise(Count = n()) %&gt;%\n  ungroup() %&gt;%                    \n  mutate(\n    Percentage = round(Count / sum(Count) * 100, 1),\n    Percentage = paste0(Percentage, \"%\")\n  ) %&gt;%\n  kable(\n    caption = \"Absorbed by Host Company\",\n    align = c(\"l\", \"r\", \"r\")\n  )\n\n\n\nAbsorbed by Host Company\n\n\nAbsorbed_Host_Company\nCount\nPercentage\n\n\n\n\nNo\n200\n92.6%\n\n\nYes\n16\n7.4%\n\n\n\n\n\nOnly 7.4% of employees were absorbed by their respective host company. However, this variable is likely skewed because the economy is saturated and businesses cannot accommodate more employees rendering this criterion relatively less useful for prediction.\n\n\nShow code\nAbsorbed_Host_Company_Means &lt;- raw_data %&gt;%\n  ungroup() %&gt;% \n  dplyr::select(c(Absorbed_Host_Company, Composite_Risk_Score)) %&gt;% \n  na.omit() %&gt;% \n  group_by(Absorbed_Host_Company) %&gt;%\n  summarise(Mean_Composite_Risk = mean(Composite_Risk_Score)) %&gt;% \n  arrange(Mean_Composite_Risk) %&gt;%\n  dplyr::mutate(Mean_Composite_Risk = round(Mean_Composite_Risk, 2))\n\nkable(Absorbed_Host_Company_Means)\n\n\n\n\n\nAbsorbed_Host_Company\nMean_Composite_Risk\n\n\n\n\nNo\n33.49\n\n\nYes\n36.68\n\n\n\n\n\nEmployees who were absorbed by their host company had a slightly higher average composite risk score.\n\n\nShow code\nggplot(raw_data,\n       aes(x = Absorbed_Host_Company,\n           y = Composite_Risk_Score,\n           fill = Absorbed_Host_Company)) +\n   geom_boxplot(outlier.shape = NA, \n                width = 0.55,\n                color = \"grey\") +\n  labs(\n    title = \"Composite Risk Score Distribution: Absorbed by Host Company\",\n    subtitle = \"Higher scores indicate greater overall risk\",\n    x = \"Absorbed by Host Company\",\n    y = \"Composite Risk Score\"\n  ) +\n  theme_minimal(base_size = 14) +\n  theme(\n    legend.position = \"none\",\n    plot.title = element_text(\n      hjust = 0.5, \n      face = \"bold\",\n      size = 16,\n    ),\n    plot.subtitle = element_text(\n      hjust = 0.5, \n      color = \"grey50\", \n      size = 12\n    ),\n    axis.title = element_text(),\n    axis.text.x = element_text(angle = 0, vjust = 0.5, size = 12),  # no angle needed for two categories\n    axis.text.y = element_text(size = 11),\n    panel.grid.major = element_blank(),      # remove major grid lines\n    panel.grid.minor = element_blank(),      # remove minor grid lines\n    panel.border = element_rect(colour = \"grey80\", fill = NA, linewidth = 0.5),\n  ) +\n  \n scale_fill_manual(values = c(\n    \"No\"  = \"#6A1B9A\",     \n    \"Yes\" = \"#9A4EAE\"     \n  )) +\n  \n  # Slight y-axis expansion for better spacing\n  scale_y_continuous(expand = expansion(mult = c(0.05, 0.1)))\n\n\n\n\n\n\n\nFurther Studies\n\n\nShow code\nraw_data %&gt;%\n  dplyr::select(Further_Studies) %&gt;%\n  na.omit() %&gt;%\n  group_by(Further_Studies) %&gt;%\n  summarise(Count = n()) %&gt;%\n  ungroup() %&gt;%                    \n  mutate(\n    Percentage = round(Count / sum(Count) * 100, 1),\n    Percentage = paste0(Percentage, \"%\")\n  ) %&gt;%\n  kable(\n    caption = \"Further Studies\",\n    align = c(\"l\", \"r\", \"r\")\n  )\n\n\n\nFurther Studies\n\n\nFurther_Studies\nCount\nPercentage\n\n\n\n\nNo\n197\n91.2%\n\n\nYes\n19\n8.8%\n\n\n\n\n\nOnly 8.8% pursued further studies however this is likely influenced by various external factors such as age, current level of education, and affordability.\n\n\nShow code\nFurther_Studies_Means &lt;- raw_data %&gt;%\n  dplyr::select(c(Further_Studies, Composite_Risk_Score)) %&gt;% \n  na.omit() %&gt;% \n  group_by(Further_Studies) %&gt;%\n  summarise(Mean_Composite_Risk = mean(Composite_Risk_Score)) %&gt;% \n  arrange(Mean_Composite_Risk) %&gt;%\n  dplyr::mutate(Mean_Composite_Risk = round(Mean_Composite_Risk, 2))\n\nkable(Further_Studies_Means)\n\n\n\n\n\nFurther_Studies\nMean_Composite_Risk\n\n\n\n\nYes\n28.51\n\n\nNo\n34.23\n\n\n\n\n\nLower risk is linked to continued education, suggesting the DRA can identify motivated, resilient candidates likely to seek growth opportunities.\n\n\nShow code\nggplot(raw_data,\n       aes(x = Further_Studies,\n           y = Composite_Risk_Score,\n           fill = Further_Studies)) +\n   geom_boxplot(outlier.shape = NA, \n                width = 0.55,\n                color = \"grey\") +\n  labs(\n    title = \"Composite Risk Score Distribution: Further Studies\",\n    subtitle = \"Higher scores indicate greater overall risk\",\n    x = \"Further Studies\",\n    y = \"Composite Risk Score\"\n  ) +\n  theme_minimal(base_size = 14) +\n  theme(\n    legend.position = \"none\",\n    plot.title = element_text(\n      hjust = 0.5, \n      face = \"bold\",\n      size = 16,\n    ),\n    plot.subtitle = element_text(\n      hjust = 0.5, \n      color = \"grey50\", \n      size = 12\n    ),\n    axis.title = element_text(),\n    axis.text.x = element_text(angle = 0, vjust = 0.5, size = 12),  # no angle needed for two categories\n    axis.text.y = element_text(size = 11),\n    panel.grid.major = element_blank(),      # remove major grid lines\n    panel.grid.minor = element_blank(),      # remove minor grid lines\n    panel.border = element_rect(colour = \"grey80\", fill = NA, linewidth = 0.5),\n  ) +\n  \n scale_fill_manual(values = c(\n    \"No\"  = \"#6A1B9A\",     \n    \"Yes\" = \"#9A4EAE\"     \n  )) +\n  \n  # Slight y-axis expansion for better spacing\n  scale_y_continuous(expand = expansion(mult = c(0.05, 0.1)))\n\n\n\n\n\n\n\nDimension-Level Descriptive Statistics\n\n\nShow code\nPC_dimension_summary &lt;- raw_data %&gt;% \n  group_by(Placement_Contract_Completion) %&gt;% \n  summarise(\n    `Core Traits` = mean(Core_Traits, na.rm = TRUE),\n    `Emotional Understanding` = mean(Emotional_Understanding, na.rm = TRUE),\n    `Judgement` = mean(Judgement, na.rm = TRUE),\n    `Principles` = mean(Principles, na.rm = TRUE),\n    `Uncertainty` = mean(Uncertainty, na.rm = TRUE)\n    )\n\n# Transpose: dimensions as rows, groups as columns\nPC_dimension_summary_t &lt;- PC_dimension_summary %&gt;%\n  column_to_rownames(var = \"Placement_Contract_Completion\") %&gt;% \n  t() %&gt;% \n  as.data.frame()\n\nkable(\n  PC_dimension_summary_t,\n  digits = 2,                  \n  caption = \"Placement Contract Completion: Mean DRA Dimension Scores\"\n)\n\n\n\nPlacement Contract Completion: Mean DRA Dimension Scores\n\n\n\nIn progress\nNo\nYes\n\n\n\n\nCore Traits\n34.46\n46.46\n31.87\n\n\nEmotional Understanding\n50.40\n42.51\n56.87\n\n\nJudgement\n43.50\n42.85\n36.73\n\n\nPrinciples\n39.73\n30.16\n39.30\n\n\nUncertainty\n29.32\n26.23\n29.90\n\n\n\n\n\nShow code\nPC_dimension_summary_long &lt;- PC_dimension_summary %&gt;%\n  pivot_longer(cols = c(2:6), \n               names_to = \"Dimensions\",   \n               values_to = \"Mean_Score\") %&gt;% \n  dplyr::mutate(Dimensions = as.factor(Dimensions))\n\nggplot(PC_dimension_summary_long, aes(x = Dimensions, y = Mean_Score, fill = Placement_Contract_Completion)) +\ngeom_bar(stat = \"identity\", position = \"dodge\") +\n  geom_text(\n    aes(label = round(Mean_Score, 2)),\n    position = position_dodge(width = 0.9),\n    vjust = -0.2, size = 3.2, color = \"black\"\n  ) + \nlabs(x = \"DRA Dimensions\", y = \"Mean Risk Score\", title = \"Mean DRA Dimension Risk Scores by Placement Contract Completion\") + \ntheme_minimal() +\n  theme(legend.position = \"right\",\n        plot.title = element_text(hjust = 0.5, face = \"bold\"),\n        panel.grid.major = element_blank(), panel.grid.minor = element_blank(), \n        axis.text.x = element_text(angle = 45, hjust = 1)) + \n  scale_fill_manual(values = c(\n    \"No\"  = \"#6A1B9A\",     \n    \"Yes\" = \"#9A4EAE\",\n    \"In progress\" = \"#F3E5F5\"\n  ))\n\n\n\n\n\n\n\nShow code\nSTC_dimension_summary &lt;- raw_data %&gt;% \n  group_by(Specialised_Training_Completion) %&gt;% \n  summarise(\n    `Core Traits` = mean(Core_Traits, na.rm = TRUE),\n    `Emotional Understanding` = mean(Emotional_Understanding, na.rm = TRUE),\n    `Judgement` = mean(Judgement, na.rm = TRUE),\n    `Principles` = mean(Principles, na.rm = TRUE),\n    `Uncertainty` = mean(Uncertainty, na.rm = TRUE)\n    )\n\n# Transpose: dimensions as rows, groups as columns\nSTC_dimension_summary_t &lt;- STC_dimension_summary %&gt;%\n  column_to_rownames(var = \"Specialised_Training_Completion\") %&gt;% \n  t() %&gt;% \n  as.data.frame()\n\nkable(\n  STC_dimension_summary_t,\n  digits = 2,                  \n  caption = \"Specialised Training Completion: Mean DRA Dimension Scores\"\n)\n\n\n\nSpecialised Training Completion: Mean DRA Dimension Scores\n\n\n\nNo\nYes\n\n\n\n\nCore Traits\n39.84\n32.94\n\n\nEmotional Understanding\n50.83\n54.36\n\n\nJudgement\n41.13\n38.55\n\n\nPrinciples\n39.63\n38.13\n\n\nUncertainty\n38.06\n27.62\n\n\n\n\n\nShow code\nSTC_dimension_summary_long &lt;- STC_dimension_summary %&gt;%\n  pivot_longer(cols = c(2:6), \n               names_to = \"Dimensions\",   \n               values_to = \"Mean_Score\") %&gt;% \n  dplyr::mutate(Dimensions = as.factor(Dimensions))\n\nggplot(STC_dimension_summary_long, aes(x = Dimensions, y = Mean_Score, fill = Specialised_Training_Completion)) +\ngeom_bar(stat = \"identity\", position = \"dodge\") +\n  geom_text(\n    aes(label = round(Mean_Score, 2)),\n    position = position_dodge(width = 0.9),\n    vjust = -0.2, size = 3.2, color = \"black\"\n  ) + \nlabs(x = \"DRA Dimensions\", y = \"Mean Risk Score\", title = \"Mean DRA Dimension Risk Scores by Specialised Training Completion\") + \ntheme_minimal() +\n  theme(legend.position = \"right\",\n        plot.title = element_text(hjust = 0.5, face = \"bold\"),\n        panel.grid.major = element_blank(), panel.grid.minor = element_blank(), \n        axis.text.x = element_text(angle = 45, hjust = 1)) + \n  scale_fill_manual(values = c(\"No\"  = \"#6A1B9A\", \"Yes\" = \"#9A4EAE\"))\n\n\n\n\n\n\n\nShow code\nEmployment_dimension_summary &lt;- raw_data %&gt;% \n  group_by(Current_Employment_Status) %&gt;% \n  summarise(\n    `Core Traits` = mean(Core_Traits, na.rm = TRUE),\n    `Emotional Understanding` = mean(Emotional_Understanding, na.rm = TRUE),\n    `Judgement` = mean(Judgement, na.rm = TRUE),\n    `Principles` = mean(Principles, na.rm = TRUE),\n    `Uncertainty` = mean(Uncertainty, na.rm = TRUE)\n    )\n\n# Transpose: dimensions as rows, groups as columns\nEmployment_dimension_summary_t &lt;- Employment_dimension_summary %&gt;%\n  column_to_rownames(var = \"Current_Employment_Status\") %&gt;%  # make group column as row names\n  t() %&gt;%  # transpose\n  as.data.frame()\n\nkable(\n  Employment_dimension_summary_t,\n  digits = 2,                  \n  caption = \"Current Employment Status: Mean DRA Dimension Scores\"\n)\n\n\n\nCurrent Employment Status: Mean DRA Dimension Scores\n\n\n\n\n\n\n\n\n\n\nFull_time\nPart_time\nTemporary_Contract\nUnemployed\n\n\n\n\nCore Traits\n39.63\n36.29\n35.13\n28.71\n\n\nEmotional Understanding\n47.63\n70.42\n53.26\n52.95\n\n\nJudgement\n39.16\n45.60\n38.01\n39.06\n\n\nPrinciples\n36.48\n37.88\n37.57\n41.12\n\n\nUncertainty\n27.93\n30.06\n29.29\n29.98\n\n\n\n\n\nShow code\nEmployment_dimension_summary_long &lt;- Employment_dimension_summary %&gt;%\n  pivot_longer(cols = c(2:6), \n               names_to = \"Dimensions\",   \n               values_to = \"Mean_Score\") %&gt;% \n  dplyr::mutate(Dimensions = as.factor(Dimensions))\n\nggplot(Employment_dimension_summary_long, \n       aes(x = Dimensions, y = Mean_Score, fill = Current_Employment_Status)) +\n  geom_bar(stat = \"identity\", position = position_dodge(width = 0.9)) +\n  labs(x = \"DRA Dimensions\", y = \"Mean Risk Score\", \n       title = \"Mean DRA Dimension Risk Scores by Employment Status\") +\n  theme_minimal() +\n  theme(legend.position = \"right\",\n        plot.title = element_text(hjust = 0.5, face = \"bold\"),\n        panel.grid.major = element_blank(), \n        panel.grid.minor = element_blank(),\n        axis.text.x = element_text(angle = 45, hjust = 1)) +\n  scale_fill_manual(values = c(\n    \"Unemployed\"       = \"#6A1B9A\",\n    \"Full_time\"        = \"#9A4EAE\",\n    \"Part_time\"        = \"#F3E5F5\",\n    \"Temporary_Contract\" = \"#7E57C2\"\n  ))\n\n\n\n\n\n\n\nShow code\nAH_dimension_summary &lt;- raw_data %&gt;% \n  group_by(Absorbed_Host_Company) %&gt;% \n  summarise(\n    `Core Traits` = mean(Core_Traits, na.rm = TRUE),\n    `Emotional Understanding` = mean(Emotional_Understanding, na.rm = TRUE),\n    `Judgement` = mean(Judgement, na.rm = TRUE),\n    `Principles` = mean(Principles, na.rm = TRUE),\n    `Uncertainty` = mean(Uncertainty, na.rm = TRUE)\n    )\n\n# Transpose: dimensions as rows, groups as columns\nAH_dimension_summary_t &lt;- AH_dimension_summary %&gt;%\n  column_to_rownames(var = \"Absorbed_Host_Company\") %&gt;% \n  t() %&gt;% \n  as.data.frame()\n\nkable(\n  AH_dimension_summary_t,\n  digits = 2,                  \n  caption = \"Absorbed by Host Company: Mean DRA Dimension Scores\"\n)\n\n\n\nAbsorbed by Host Company: Mean DRA Dimension Scores\n\n\n\nNo\nYes\n\n\n\n\nCore Traits\n34.44\n29.69\n\n\nEmotional Understanding\n53.34\n59.16\n\n\nJudgement\n39.20\n36.23\n\n\nPrinciples\n38.33\n39.06\n\n\nUncertainty\n30.82\n11.10\n\n\n\n\n\nShow code\nAH_dimension_summary_long &lt;- AH_dimension_summary %&gt;%\n  pivot_longer(cols = c(2:6), \n               names_to = \"Dimensions\",   \n               values_to = \"Mean_Score\") %&gt;% \n  dplyr::mutate(Dimensions = as.factor(Dimensions))\n\nggplot(AH_dimension_summary_long, aes(x = Dimensions, y = Mean_Score, fill = Absorbed_Host_Company)) +\ngeom_bar(stat = \"identity\", position = \"dodge\") +\ngeom_text(\n    aes(label = round(Mean_Score, 2)),\n    position = position_dodge(width = 0.9),\n    vjust = -0.2, size = 3.2, color = \"black\"\n  ) + \nlabs(x = \"DRA Dimensions\", y = \"Mean Risk Score\", title = \"Mean DRA Dimension Risk Scores - Absorbed by Host Company\") + \ntheme_minimal() +\n  theme(legend.position = \"right\",\n        plot.title = element_text(hjust = 0.5, face = \"bold\"),\n        panel.grid.major = element_blank(), panel.grid.minor = element_blank(), \n        axis.text.x = element_text(angle = 45, hjust = 1)) +\n  scale_fill_manual(values = c(\"No\"  = \"#6A1B9A\", \"Yes\" = \"#9A4EAE\"))\n\n\n\n\n\n\n\nShow code\nFS_dimension_summary &lt;- raw_data %&gt;% \n  group_by(Further_Studies) %&gt;% \n  summarise(\n    `Core Traits` = mean(Core_Traits, na.rm = TRUE),\n    `Emotional Understanding` = mean(Emotional_Understanding, na.rm = TRUE),\n    `Judgement` = mean(Judgement, na.rm = TRUE),\n    `Principles` = mean(Principles, na.rm = TRUE),\n    `Uncertainty` = mean(Uncertainty, na.rm = TRUE)\n    )\n\n# Transpose: dimensions as rows, groups as columns\nFS_dimension_summary_t &lt;- FS_dimension_summary %&gt;%\n  column_to_rownames(var = \"Further_Studies\") %&gt;% \n  t() %&gt;% \n  as.data.frame()\n\nkable(\n  FS_dimension_summary_t,\n  digits = 2,                  \n  caption = \"Further Studies: Mean DRA Dimension Scores\"\n)\n\n\n\nFurther Studies: Mean DRA Dimension Scores\n\n\n\nNo\nYes\n\n\n\n\nCore Traits\n34.38\n31.12\n\n\nEmotional Understanding\n54.08\n50.60\n\n\nJudgement\n39.17\n37.00\n\n\nPrinciples\n38.62\n35.96\n\n\nUncertainty\n28.46\n38.72\n\n\n\n\n\nShow code\nFS_dimension_summary_long &lt;- FS_dimension_summary %&gt;%\n  pivot_longer(cols = c(2:6), \n               names_to = \"Dimensions\",   \n               values_to = \"Mean_Score\") %&gt;% \n  dplyr::mutate(Dimensions = as.factor(Dimensions))\n\nggplot(FS_dimension_summary_long, aes(x = Dimensions, y = Mean_Score, fill = Further_Studies)) +\ngeom_bar(stat = \"identity\", position = \"dodge\") +\n  geom_text(\n    aes(label = round(Mean_Score, 2)),\n    position = position_dodge(width = 0.9),\n    vjust = -0.2, size = 3.2, color = \"black\"\n  ) + \nlabs(x = \"DRA Dimensions\", y = \"Mean Risk Score\", title = \"Mean DRA Dimension Risk Scores by Further Studies\") + \ntheme_minimal() +\n  theme(legend.position = \"right\",\n        plot.title = element_text(hjust = 0.5, face = \"bold\"),\n        panel.grid.major = element_blank(), panel.grid.minor = element_blank(), \n        axis.text.x = element_text(angle = 45, hjust = 1)) + \n  scale_fill_manual(values = c(\"No\"  = \"#6A1B9A\", \"Yes\" = \"#9A4EAE\"))\n\n\n\n\n\n\n\nHandling Class Imbalance with SMOTE\nClass imbalance is a common and often critical challenge in predictive modelling and data science.\nIn many cases, the number of instances in one class (the minority class) is significantly smaller than the other (the majority class).\nCommon examples include:\n\nAdverse drug reaction prediction: Serious reactions occur in a tiny percentage of patients.\nCredit card fraud: ~0.1–0.5% of transactions are fraudulent (minority class = fraud), while 99.5%+ are legitimate.\nEquipment failure prediction: Machine breakdowns or defects occur in &lt;1–5% of operational cycles.\n\nThis imbalance (i.e., where one outcome is typically less frequent than another) is natural and expected in real-world data. SMOTE (or other balancing techniques) is frequently applied in these cases to help models learn meaningful patterns from the minority class without being overwhelmed by the majority.\n\nSynthetic Minority Over-sampling Technique (SMOTE)\nTo address class imbalance before running predictive models, we applied the Synthetic Minority Over-sampling Technique (SMOTE).\nSMOTE works by:\n\nIdentifying minority class samples.\nFor each minority sample, finding its k nearest neighbours in feature space (typically k=5).\nGenerating new synthetic data points along the line segments joining the minority sample to its neighbours.\nRepeating this process until the classes are (approximately) equally represented.\n\nThis approach:\n\nIncreases the number of minority examples without simply duplicating existing ones (which can cause over-fitting).\nCreates realistic synthetic samples that lie within the distribution of the original data.\nImproves model learning on rare classes without discarding majority-class information.\n\nBy balancing the classes with SMOTE prior to training models, we ensure more reliable and fair performance evaluation, particularly for minority outcomes such as non-completion of specialized training or not pursuing further studies.\n\n\nShow code\n# Each of the outcome variables is treated separately as the target for SMOTE.\n# After applying SMOTE for one target, you get a new balanced data set only for that target. Thus, you end up with five separate balanced data sets (one for each outcome variable).\n\n# Drop empty levels after filtering. \nsmote_data &lt;- raw_data %&gt;%\n  mutate(Current_Employment_Status = droplevels(Current_Employment_Status))\n\n#  Apply SMOTE for each target variable separately\n# ────────────────────────────────────────────────────────────────\n# 1. Placement_Contract_Completion \n\nset.seed(2025)\n\nbalanced_contract &lt;- smotenc(\n  df     = smote_data,                        # full data frame\n  var    = \"Placement_Contract_Completion\",   # target column name (must be factor)\n  k      = 5,                                            # nearest neighbors\n  over_ratio = 1                                         # 1 = balance to 1:1; &gt;1 oversamples minority more\n)\n\nkable(balanced_contract %&gt;%\n  dplyr::select(c(Placement_Contract_Completion)) %&gt;% \n  group_by(Placement_Contract_Completion) %&gt;%\n  dplyr::summarise(count = n())) \n\n\n\n\n\nPlacement_Contract_Completion\ncount\n\n\n\n\nIn progress\n142\n\n\nNo\n142\n\n\nYes\n142\n\n\n\n\n\nAfter applying SMOTE, all classes (i.e., In progress; No; Yes) for Placement Contract Completion are equally represented.\n\n\nShow code\nset.seed(2025)\n\nbalanced_training &lt;- smotenc(\n  df     = smote_data,                           \n  var    = \"Specialised_Training_Completion\",    \n  k      = 5,                                           \n  over_ratio = 1                                         \n)\n\nkable(balanced_training %&gt;%\n  dplyr::select(c(Specialised_Training_Completion)) %&gt;% \n  group_by(Specialised_Training_Completion) %&gt;%\n  dplyr::summarise(count = n()))\n\n\n\n\n\nSpecialised_Training_Completion\ncount\n\n\n\n\nNo\n180\n\n\nYes\n180\n\n\n\n\n\n\n\nShow code\nset.seed(2025)\n\nbalanced_employment_status &lt;- smotenc(\n  df     = smote_data,                           \n  var    = \"Current_Employment_Status\",    \n  k      = 5,                                           \n  over_ratio = 1                                         \n)\n\nkable(balanced_employment_status %&gt;%\n  dplyr::select(c(Current_Employment_Status)) %&gt;% \n  group_by(Current_Employment_Status) %&gt;%\n  dplyr::summarise(count = n())) \n\n\n\n\n\nCurrent_Employment_Status\ncount\n\n\n\n\nFull_time\n118\n\n\nPart_time\n118\n\n\nTemporary_Contract\n118\n\n\nUnemployed\n118\n\n\n\n\n\n\n\nShow code\nset.seed(2025)\n\nbalanced_studies &lt;- smotenc(\n  df     = smote_data,                           \n  var    = \"Further_Studies\",    \n  k      = 5,                                           \n  over_ratio = 1                                         \n)\n\nkable(balanced_studies %&gt;%\n  dplyr::select(c(Further_Studies)) %&gt;% \n  group_by(Further_Studies) %&gt;%\n  dplyr::summarise(count = n())) \n\n\n\n\n\nFurther_Studies\ncount\n\n\n\n\nNo\n197\n\n\nYes\n197"
  },
  {
    "objectID": "psychometrics-candidate-outcomes.html#analysis-of-predictive-performance-using-roc-curves",
    "href": "psychometrics-candidate-outcomes.html#analysis-of-predictive-performance-using-roc-curves",
    "title": "Psychometric Risk Assessment: Employment & Placement Outcomes",
    "section": "Analysis of Predictive Performance Using ROC Curves",
    "text": "Analysis of Predictive Performance Using ROC Curves\nLogistic regression models were used to examine the extent to which three DRA variables (i.e., predictors) could discriminate between two binary outcomes.\nTwo outcomes selected for this analysis:\n\nSpecialised Training Completion (Yes / No)\nFurther Studies (Yes / No)\n\n\nPredictors Used\nThe models use these three variables:\n\nComposite Risk Score\nPrinciples\nEmotional Understanding\n\nFor each outcome, the data were split into training (80%) and test (20%) sets using stratified sampling to maintain class proportions. Logistic regression models were fitted on the training data, and performance was evaluated on the held-out test set.\n\nKey Metric Definitions in Context\n\nAUC (Area Under the ROC Curve): Measures overall ability to rank candidates correctly. AUC = 0.5 means no discrimination (random guessing); AUC &gt; 0.7–0.8 is generally useful in applied settings.\nAccuracy: Overall % correct predictions (TP + TN) / total\nPrecision (for Yes): Of all candidates predicted to complete training / pursue further studies, what % were actually Yes? (TP / (TP + FP))\nRecall (for Yes): What proportion of actual Yes cases does the model correctly identify as Yes? Formula: TP / (TP + FN)\n\n\n\nShow code\npredictors &lt;- c(\"Composite_Risk_Score\", \"Principles\", \"Emotional_Understanding\") \n\nevaluate_binary_outcome &lt;- function(outcome) {\n  \n  cat(\"\\n═══════════════════════════════════════════════════════════════\\n\")\n  cat(\"Binary Outcome:\", outcome$name, \"\\n\")\n  cat(\"═══════════════════════════════════════════════════════════════\\n\\n\")\n  \n  df &lt;- outcome$data\n  target_var &lt;- outcome$name\n  pos &lt;- outcome$positive\n  neg &lt;- setdiff(levels(df[[target_var]]), pos)\n  \n  df[[target_var]] &lt;- as.factor(df[[target_var]])\n  \n  # Train/test split\n  set.seed(2025)\n  train_idx &lt;- createDataPartition(df[[target_var]], p = 0.8, list = FALSE)\n  train &lt;- df[train_idx, ]\n  test  &lt;- df[-train_idx, ]\n  \n  # Logistic regression\n  formula &lt;- as.formula(paste(target_var, \"~\", paste(predictors, collapse = \" + \")))\n  model &lt;- glm(formula, data = train, family = \"binomial\")\n  \n  # Predictions\n  pred_prob &lt;- predict(model, newdata = test, type = \"response\")\n  pred_class &lt;- factor(ifelse(pred_prob &gt;= 0.5, pos, neg), levels = c(neg, pos))\n  \n  # ROC\n  roc_obj &lt;- roc(\n    response  = test[[target_var]],\n    predictor = pred_prob,\n    levels    = c(neg, pos),\n    direction = \"&lt;\"\n  )\n  \n  # ggplot ROC\n  library(ggplot2)\n  p &lt;- ggroc(roc_obj, legacy.axes = TRUE) +\n    geom_abline(slope = 1, intercept = 0, linetype = \"dashed\", color = \"grey50\") +\n    labs(\n      title = paste(\"ROC Curve -\", outcome$name),\n      subtitle = paste(\"Positive class:\", pos),\n      x = \"1 - Specificity\",\n      y = \"Sensitivity\"\n    ) +\n    annotate(\"text\", x = 0.75, y = 0.25,\n             label = paste(\"AUC =\", round(auc(roc_obj), 3)),\n             size = 5, color = \"black\") +\n    theme_minimal(base_size = 13) +\n    coord_equal(ratio = 1)\n  \n  print(p)\n  \n  cat(\"AUC:\", round(auc(roc_obj), 3), \"\\n\\n\")\n  \n  # Confusion matrix\n  cm &lt;- conf_mat(\n    tibble(truth = test[[target_var]], estimate = pred_class),\n    truth = truth, estimate = estimate\n  )\n  \n# Extract the table \ntab &lt;- cm$table\n\n# Compute key metrics \ntn &lt;- tab[\"No\", \"No\"]\nfp &lt;- tab[\"Yes\", \"No\"]\nfn &lt;- tab[\"No\", \"Yes\"]\ntp &lt;- tab[\"Yes\", \"Yes\"]\n\ntotal &lt;- sum(tab)\naccuracy &lt;- (tp + tn) / total\nprecision &lt;- tp / (tp + fp)     # PPV for \"Yes\"\nrecall &lt;- tp / (tp + fn)        # sensitivity for \"Yes\"\nf1 &lt;- 2 * (precision * recall) / (precision + recall)\n\ncat(\"\\nConfusion Matrix:\\n\")\nprint(tab)\n\ncat(\"\\nAccuracy:   \", round(accuracy, 3), \"\\n\")\ncat(\"Precision:  \", round(precision, 3), \" (for Yes)\\n\")\ncat(\"Recall:     \", round(recall, 3), \" (for Yes)\\n\")\ncat(\"F1-score:   \", round(f1, 3), \"\\n\\n\")\n\n  invisible(list(model = model, roc = roc_obj, cm = cm, metrics = metrics))\n}\n\n\n\n\nShow code\n# Binary ones\nbinary_outcome1 &lt;- list(\n  list(name = \"Specialised_Training_Completion\", \n       data = balanced_training, \n       positive = \"Yes\")) \n\nresult_binary_outcome1 &lt;- lapply(binary_outcome1, evaluate_binary_outcome)\n\n\n\n═══════════════════════════════════════════════════════════════\nBinary Outcome: Specialised_Training_Completion \n═══════════════════════════════════════════════════════════════\n\n\n\n\n\nAUC: 0.529 \n\n\nConfusion Matrix:\n          Truth\nPrediction No Yes\n       No  15  14\n       Yes 21  22\n\nAccuracy:    0.514 \nPrecision:   0.512  (for Yes)\nRecall:      0.611  (for Yes)\nF1-score:    0.557 \n\n\n\n\n\nSpecialised Training Completion (Yes/No)\nAUC: 0.529 (near-random discrimination)\n\nMetrics\n\nAccuracy: 0.514 (correct classifications ~51%)\nPrecision (Yes): 0.512 — Of predicted Yes, 51.2% were actual Yes\nRecall (Yes): 0.611 — Caught 61.1% of actual Yes cases\nF1-score (Yes): 0.557 — Balanced precision/recall\n\n\n\nConfusion Matrix Breakdown\nThis shows how the model classified the test set (using 0.5 probability threshold).\n\nTrue Negatives (TN): 15 candidates correctly predicted as No\nFalse Negatives (FN): 14 candidates wrongly predicted as No (actually Yes)\nFalse Positives (FP): 21 candidates wrongly predicted as Yes (actually No)\nTrue Positives (TP): 22 candidates correctly predicted as Yes\n\nInsight: The model struggles to distinguish training completion. This outcome may depend more on programme factors than initial risk scores.\n\n\nShow code\nbinary_outcome2 &lt;- list(\n  list(name = \"Further_Studies\",                 \n       data = balanced_studies,   \n       positive = \"Yes\"))\n\n\nresult_binary_outcome2 &lt;- lapply(binary_outcome2, evaluate_binary_outcome)\n\n\n\n═══════════════════════════════════════════════════════════════\nBinary Outcome: Further_Studies \n═══════════════════════════════════════════════════════════════\n\n\n\n\n\nAUC: 0.512 \n\n\nConfusion Matrix:\n          Truth\nPrediction No Yes\n       No  16  15\n       Yes 23  24\n\nAccuracy:    0.513 \nPrecision:   0.511  (for Yes)\nRecall:      0.615  (for Yes)\nF1-score:    0.558 \n\n\n\n\n\nFurther Studies (Yes/No)\nAUC: 0.512 (near-random discrimination)\n\nMetrics\n\nAccuracy: 0.513 (correct classifications ~51%)\nPrecision (Yes): 0.511 — Of predicted Yes, 51.1% were actual Yes\nRecall (Yes): 0.615 — Caught 61.5% of actual Yes cases\nF1-score (Yes): 0.558 — Balanced precision/recall\n\n\n\nConfusion Matrix Breakdown\nThis shows how the model classified the test set (using 0.5 probability threshold).\n\nTrue Negatives (TN): 16 candidates correctly predicted as No\nFalse Negatives (FN): 15 candidates wrongly predicted as No (actually Yes)\nFalse Positives (FP): 23 candidates wrongly predicted as Yes (actually No)\nTrue Positives (TP): 24 candidates correctly predicted as Yes\n\nInsight: Similar to training completion, the three predictors explain almost nothing about whether employees pursue further studies. While ROC curves provide a strong initial assessment of how well predictors separate classes, we extended this analysis to random forest modelling as this approach incorporates multiple predictors, captures non-linear relationships and interactions, and provides robust predictions through ensemble averaging and built-in variable importance rankings."
  },
  {
    "objectID": "psychometrics-candidate-outcomes.html#predictive-modeling---random-forest",
    "href": "psychometrics-candidate-outcomes.html#predictive-modeling---random-forest",
    "title": "Psychometric Risk Assessment: Employment & Placement Outcomes",
    "section": "Predictive Modeling - Random Forest",
    "text": "Predictive Modeling - Random Forest\nRandom forests, which combine multiple decision trees to improve predictive accuracy and reduce over-fitting, are a powerful ensemble machine learning method for classification and regression.\n\nRandom Forest Model 1 - Predicting Further Studies (binary)\nA binary random forest model (2000 trees) was trained to predict whether candidates pursued further studies after placement, using three key predictors: Composite Risk Score, Principles, and Emotional Understanding. The model was evaluated on a held-out 30% test set.\n\n\nShow code\n# Binary Random Forest for Further Studies\n\n# Prepare data\nbalanced_studies_RF_df &lt;- balanced_studies\npredictors &lt;- c(\"Composite_Risk_Score\", \"Principles\", \"Emotional_Understanding\")\n\n# Train/test split (70/30) – reproducible\nset.seed(2025)\ntrain_index &lt;- createDataPartition(\n  balanced_studies_RF_df$Further_Studies,\n  p = 0.7,\n  list = FALSE\n)\ntrain_data &lt;- balanced_studies_RF_df[train_index, ]\ntest_data  &lt;- balanced_studies_RF_df[-train_index, ]\n\n# Fit binary random forest\nrf_model_binary &lt;- randomForest(\n  x = train_data[, predictors],\n  y = train_data$Further_Studies,\n  ntree = 2000,\n  mtry = floor(sqrt(length(predictors))),\n  importance = TRUE,\n  proximity = FALSE,\n  na.action = na.roughfix,\n  seed = 2025 \n)\n\n# ── Predictions on test data ──────────────────────────────────────────────────────\nrf_pred_class &lt;- predict(rf_model_binary, newdata = test_data[, predictors])\nrf_pred_prob  &lt;- predict(rf_model_binary, newdata = test_data[, predictors], type = \"prob\")[, \"Yes\"]  \n\n# ── Confusion Matrix (binary) ─────────────────────────────────────────────────────\ncm_caret &lt;- confusionMatrix(\n  data      = rf_pred_class,\n  reference = test_data$Further_Studies,\n  positive  = \"Yes\" \n)\n\ncat(\"\\nConfusion Matrix (Test Set):\\n\")\n\n\n\nConfusion Matrix (Test Set):\n\n\nShow code\nprint(cm_caret$table)\n\n\n          Reference\nPrediction No Yes\n       No  42  12\n       Yes 17  47\n\n\n\nConfusion Matrix Breakdown\nThis shows how the model classified the test set (using 0.5 probability threshold).\n\nTrue Negatives (TN): 42 candidates correctly predicted as No\nFalse Negatives (FN): 12 candidates wrongly predicted as No (actually Yes)\nFalse Positives (FP): 17 candidates wrongly predicted as Yes (actually No)\nTrue Positives (TP): 47 candidates correctly predicted as Yes\n\n\n\nShow code\n# Overall stats\ncat(\"\\nOverall Accuracy (Test Set):\", round(cm_caret$overall[\"Accuracy\"], 3), \"\\n\")\n\n\n\nOverall Accuracy (Test Set): 0.754 \n\n\nShow code\n# Binary metrics (for Yes class)\ncat(\"\\nMetrics for 'Yes' class:\\n\")\n\n\n\nMetrics for 'Yes' class:\n\n\nShow code\ncat(\"Precision:\", round(cm_caret$byClass[\"Pos Pred Value\"], 3), \"\\n\")\n\n\nPrecision: 0.734 \n\n\nShow code\ncat(\"Recall (Sensitivity):\", round(cm_caret$byClass[\"Sensitivity\"], 3), \"\\n\")\n\n\nRecall (Sensitivity): 0.797 \n\n\nShow code\ncat(\"F1-score:\", round(cm_caret$byClass[\"F1\"], 3), \"\\n\")\n\n\nF1-score: 0.764 \n\n\n\n\nKey Performance Metrics (for the “Yes” class)\n\nOverall Accuracy: 75.4% correct classifications.\nPrecision (Yes): Of predicted “Yes”, 73.4% actually pursued further studies.\nRecall (Sensitivity for Yes): Caught 79.7% of actual “Yes” cases.\nF1-score (Yes): 0.764 — Balanced measure of precision and recall.\n\n\n\nShow code\n# ROC Curve & AUC\nroc_obj &lt;- roc(\n  response  = test_data$Further_Studies,\n  predictor = rf_pred_prob,\n  levels    = c(\"No\", \"Yes\"),\n  direction = \"&lt;\"\n)\n\ncat(\"\\nAUC (Test Set):\", round(auc(roc_obj), 3), \"\\n\")\n\n\n\nAUC (Test Set): 0.881 \n\n\nShow code\n# Plot ROC\nggroc(roc_obj, legacy.axes = TRUE) +\n  geom_abline(slope = 1, intercept = 0, linetype = \"dashed\", color = \"grey50\") +\n  labs(\n    title = \"ROC Curve – Further Studies Prediction\",\n    subtitle = paste(\"AUC =\", round(auc(roc_obj), 3)),\n    x = \"1 – Specificity\",\n    y = \"Sensitivity\"\n  ) +\n  annotate(\"text\", x = 0.75, y = 0.25,\n           label = paste(\"AUC =\", round(auc(roc_obj), 3)),\n           size = 5, color = \"black\") +\n  theme_minimal(base_size = 13) +\n  coord_equal(ratio = 1)\n\n\n\n\n\n\nAUC: 0.881 — strong discriminative ability.\n\n\n\nVariable Importance\nVariable importance in random forests quantifies how much each predictor contributes to the model’s performance.\nHow Variable Importance is Calculated:\n\nGini Impurity: At each node split in a tree, Gini measures how “pure” the classes are after the split (lower Gini = better split).\nMeanDecreaseGini: For each predictor, sum the total Gini decrease across all splits where it was used, then average over all trees. High value = the feature frequently leads to strong purity improvements (i.e., it’s informative for classification).\nMeanDecreaseAccuracy: Permutes the feature and measures drop in accuracy (another view of importance).\n\nHow This Increases Prediction:\n\nFeature Selection: High-importance variables (e.g. Emotional_Understanding at 51.2) are the “weights” — they drive splits more often, making the model focus on what matters, improving accuracy.\nReduces Noise: Low-importance features (e.g. Principles at 42.3) contribute less, preventing over-fitting to irrelevant signals.\n\nOverall, this boosts prediction by prioritizing strong discriminators leading to more reliable outcomes.\n\n\nShow code\n# Variable Importance \ncat(\"\\nVariable Importance (MeanDecreaseGini):\\n\")\n\n\n\nVariable Importance (MeanDecreaseGini):\n\n\nShow code\nrandomForest::importance(rf_model_binary) %&gt;%\n  as.data.frame() %&gt;%\n  rownames_to_column(\"Predictor\") %&gt;%\n  arrange(desc(MeanDecreaseGini)) %&gt;%\n  print()\n\n\n                Predictor       No      Yes MeanDecreaseAccuracy\n1 Emotional_Understanding 37.90463 85.05574             83.05474\n2    Composite_Risk_Score 20.85612 72.01960             65.53884\n3              Principles 20.50967 58.74359             57.65015\n  MeanDecreaseGini\n1         51.24533\n2         43.95156\n3         42.30505\n\n\nThe plot below shows MeanDecreaseGini — a measure of how much each predictor reduces node impurity (class mixing) across all 2000 trees in the random forest. Higher values indicate greater importance / weight in predicting pursuit of further studies.\n\n\nShow code\n# Plot importance\nrandomForest::varImpPlot(\n  rf_model_binary,\n  main = \"Random Forest Variable Importance – Further Studies\", \n  type = 2\n)\n\n\n\n\n\n\n\n\nComparison: ROC Curve vs Random Forest for Further Studies\nThe random forest model achieved a substantially higher AUC of 0.881, overall accuracy of 75.4%, precision of 73.4% and recall of 79.7% for the “Yes” class.\nWhy the boost in prediction?\nThe boost in prediction from the random forest (AUC 0.881 vs. ROC/logistic regression’s 0.512) stems from its ability to model the same predictors more effectively than logistic regression. While logistic regression assumes linear relationships and can struggle with interactions or non-linear patterns, random forest builds an ensemble of 2000 decision trees that capture complex, non-linear effects, feature interactions, and variability — leading to greater robustness, reduced over-fitting, and improved generalization on the test set.\n\n\nRandom Forest Model 2: Predicting Employment Status (multi-class)\nA random forest model (2000 trees) was trained to predict Employment Status using the following predictors: Core Traits, Emotional Understanding, Judgement, Principles, Uncertainty, and Composite Risk Score.\n\n\nShow code\nbalanced_employment_status_RF_df &lt;- balanced_employment_status   \n\npredictors &lt;- c(\"Core_Traits\", \"Emotional_Understanding\", \"Judgement\", \"Principles\", \"Uncertainty\", \"Composite_Risk_Score\")  \n\n############################################################\n# Train / test split (70 / 30) \n############################################################\nset.seed(2025)\n\ntrain_index &lt;- createDataPartition(\n  balanced_employment_status_RF_df$Current_Employment_Status, \n  p = 0.7, \n  list = FALSE\n)\n\ntrain_data &lt;- balanced_employment_status_RF_df[train_index, ]\ntest_data  &lt;- balanced_employment_status_RF_df[-train_index, ]\n\n############################################################\n# Fit Random Forest model (multi-class)\n############################################################\nrf_model &lt;- randomForest(\n  x = train_data[, predictors],          \n  y = train_data$Current_Employment_Status,\n  ntree = 2000,                             # number of trees\n  mtry = floor(sqrt(length(predictors))),   # default for classification\n  importance = TRUE,                        # for variable importance\n  proximity = FALSE,                     \n  na.action = na.roughfix, \n  seed = 2025\n)\n\n############################################################\n# Predictions on test data\n############################################################\nrf_pred_class &lt;- predict(rf_model, newdata = test_data[, predictors])\n\nrf_pred_prob &lt;- predict(\n  rf_model,\n  newdata = test_data[, predictors],\n  type = \"prob\"\n)\n\n############################################################\n# Confusion matrix & performance metrics\n############################################################\ncm_caret &lt;- confusionMatrix(\n  data      = rf_pred_class,\n  reference = test_data$Current_Employment_Status\n)\n\ncat(\"\\nConfusion Matrix (Test Set):\\n\")\n\n\n\nConfusion Matrix (Test Set):\n\n\nShow code\nprint(cm_caret$table)\n\n\n                    Reference\nPrediction           Full_time Part_time Temporary_Contract Unemployed\n  Full_time                 24         4                  6          6\n  Part_time                  4        30                  7          1\n  Temporary_Contract         3         0                 14          6\n  Unemployed                 4         1                  8         22\n\n\n\nInterpretation of the Confusion Matrix (Test Set)\nThe confusion matrix shows how the random forest model classified employment status on the held-out test set (using majority vote):\n\nRows = predicted class (model’s guess)\nColumns = actual (true) class\n\nKey observations:\n\nFull time was predicted reasonably well (24 correctly predicted out of 35 actual cases).\nPart time showed the strongest performance (30 correctly predicted out of 35 actual cases).\nTemporary Contract had the weakest recall (only 14 correctly predicted out of 35 actual cases).\nUnemployed was moderately identified (22 correctly predicted out of 35 actual cases).\n\n\n\nShow code\n# Overall accuracy\ncat(\"\\nOverall Accuracy:\", round(cm_caret$overall[\"Accuracy\"], 3), \"\\n\")\n\n\n\nOverall Accuracy: 0.643 \n\n\nShow code\n# Per-class precision, recall, F1 \ncat(\"\\nPer-class metrics:\\n\")\n\n\n\nPer-class metrics:\n\n\nShow code\nprint(cm_caret$byClass[, c(\"Precision\", \"Recall\", \"F1\")])\n\n\n                          Precision    Recall        F1\nClass: Full_time          0.6000000 0.6857143 0.6400000\nClass: Part_time          0.7142857 0.8571429 0.7792208\nClass: Temporary_Contract 0.6086957 0.4000000 0.4827586\nClass: Unemployed         0.6285714 0.6285714 0.6285714\n\n\nShow code\n# Macro-averaged metrics (simple average across classes)\nmacro_precision &lt;- mean(cm_caret$byClass[, \"Precision\"], na.rm = TRUE)\nmacro_recall    &lt;- mean(cm_caret$byClass[, \"Recall\"], na.rm = TRUE)\nmacro_f1        &lt;- mean(cm_caret$byClass[, \"F1\"], na.rm = TRUE)\n\ncat(\"\\nMacro-averaged:\\n\")\n\n\n\nMacro-averaged:\n\n\nShow code\ncat(\"Precision:\", round(macro_precision, 3), \"\\n\")\n\n\nPrecision: 0.638 \n\n\nShow code\ncat(\"Recall:   \", round(macro_recall,    3), \"\\n\")\n\n\nRecall:    0.643 \n\n\nShow code\ncat(\"F1:       \", round(macro_f1,        3), \"\\n\")\n\n\nF1:        0.633 \n\n\nShow code\n############################################################\n# Multi-class AUC (macro-averaged)\n############################################################\nmulti_auc &lt;- multiclass.roc(\n  response  = test_data$Current_Employment_Status,\n  predictor = rf_pred_prob\n)\n\ncat(\"\\nMulti-class AUC (macro-averaged):\", round(auc(multi_auc), 3), \"\\n\")\n\n\n\nMulti-class AUC (macro-averaged): 0.875 \n\n\nAUC = 0.875 → very good discrimination ability.\nThis means that if you pick one person from each class at random, the model assigns higher probability to the correct class 87.5% of the time. In other words, the model is very good at assigning higher probabilities to the correct class, even when it makes classification errors.\n\n\nShow code\n############################################################\n# Variable importance\n############################################################\ncat(\"\\nVariable Importance (MeanDecreaseGini):\\n\")\n\n\n\nVariable Importance (MeanDecreaseGini):\n\n\nShow code\nrandomForest::importance(rf_model) %&gt;%\n  as.data.frame() %&gt;%\n  rownames_to_column(\"Predictor\") %&gt;%\n  arrange(desc(MeanDecreaseGini)) %&gt;%\n  head(15) %&gt;%               \n  print()\n\n\n                Predictor Full_time Part_time Temporary_Contract Unemployed\n1 Emotional_Understanding  89.30430  98.76786         11.9130505   23.82122\n2              Principles  71.66467  73.81713          5.6151620   34.65674\n3    Composite_Risk_Score  66.43196  62.26855         -0.8070441   47.36499\n4             Core_Traits  62.68984  53.81674         -5.4520896   33.14145\n5               Judgement  50.70257  62.13475         -1.9497412   16.89340\n6             Uncertainty  39.98729  56.67910          1.4615802   23.09460\n  MeanDecreaseAccuracy MeanDecreaseGini\n1            111.17026         51.07874\n2             90.07002         47.53747\n3             88.58889         46.47894\n4             71.82570         40.47700\n5             70.54355         39.95138\n6             60.60349         22.74974\n\n\nThe plot below shows MeanDecreaseGini — a measure of how much each predictor reduces node impurity (class mixing) across all 2000 trees in the random forest. Higher values indicate greater importance / weight in predicting Employment Status.\nTop predictors:\n\nEmotional Understanding (51.1) — strongest overall driver\nPrinciples (47.5)\nComposite Risk Score (46.5)\n\nThese three features carry the majority of predictive power, suggesting that they consistently help separate employment classes.\n\n\nShow code\nrandomForest::varImpPlot(\n  rf_model,\n  main = \"Random Forest Variable Importance - Current Employment Status\",\n  n.var = 6,               # show top 6 predictors. \n  type = 2                 # type = 2 for Gini. \n)\n\n\n\n\n\n\n\n\nRandom Forest Model 3: Predicting Work Placement Contract (multi-class)\nA random forest model (2000 trees) was trained to predict Work Placement Contract Completion using the same six predictors: Core Traits, Emotional Understanding, Judgement, Principles, Uncertainty, and Composite Risk Score.\n\n\nShow code\n# Random Forests for Work_Placement_Contract \n\nbalanced_contract_RF_df &lt;- balanced_contract  \n\npredictors &lt;- c(\"Core_Traits\", \"Emotional_Understanding\", \"Judgement\", \"Principles\", \"Uncertainty\", \"Composite_Risk_Score\")  \n\n############################################################\n# Train / test split (70 / 30) \n############################################################\nset.seed(2025)\n\ntrain_index &lt;- createDataPartition(\n  balanced_contract_RF_df$Placement_Contract_Completion, \n  p = 0.7, \n  list = FALSE\n)\n\ntrain_data &lt;- balanced_contract_RF_df[train_index, ]\ntest_data  &lt;- balanced_contract_RF_df[-train_index, ]\n\n############################################################\n# Fit Random Forest model (multi-class)\n############################################################\nrf_model &lt;- randomForest(\n  x = train_data[, predictors],          \n  y = train_data$Placement_Contract_Completion,\n  ntree = 2000,                             # number of trees\n  mtry = floor(sqrt(length(predictors))),   # default for classification\n  importance = TRUE,                        # for variable importance\n  proximity = FALSE,                     \n  na.action = na.roughfix,\n  seed = 2025\n)\n\n############################################################\n# Predictions on test data\n############################################################\nrf_pred_class &lt;- predict(rf_model, newdata = test_data[, predictors])\n\nrf_pred_prob &lt;- predict(\n  rf_model,\n  newdata = test_data[, predictors],\n  type = \"prob\"\n)\n\n############################################################\n# Confusion matrix & performance metrics\n############################################################\ncm_caret &lt;- confusionMatrix(\n  data      = rf_pred_class,\n  reference = test_data$Placement_Contract_Completion\n)\n\ncat(\"\\nConfusion Matrix (Test Set):\\n\")\n\n\n\nConfusion Matrix (Test Set):\n\n\nShow code\nprint(cm_caret$table)\n\n\n             Reference\nPrediction    In progress No Yes\n  In progress          29  3  14\n  No                    3 33   5\n  Yes                  10  6  23\n\n\n\nInterpretation of the Confusion Matrix (Test Set)\n\nIn progress: 29 correctly predicted out of 42 actual cases.\nNo: 33 correctly predicted out of 42 actual cases.\nYes: 23 correctly predicted out of 42 actual cases.\n\n\n\nShow code\n# Overall accuracy\ncat(\"\\nOverall Accuracy:\", round(cm_caret$overall[\"Accuracy\"], 3), \"\\n\")\n\n\n\nOverall Accuracy: 0.675 \n\n\nShow code\n# Per-class precision, recall, F1 \ncat(\"\\nPer-class metrics:\\n\")\n\n\n\nPer-class metrics:\n\n\nShow code\nprint(cm_caret$byClass[, c(\"Precision\", \"Recall\", \"F1\")])\n\n\n                   Precision    Recall        F1\nClass: In progress 0.6304348 0.6904762 0.6590909\nClass: No          0.8048780 0.7857143 0.7951807\nClass: Yes         0.5897436 0.5476190 0.5679012\n\n\nShow code\n# Macro-averaged metrics (simple average across classes)\nmacro_precision &lt;- mean(cm_caret$byClass[, \"Precision\"], na.rm = TRUE)\nmacro_recall    &lt;- mean(cm_caret$byClass[, \"Recall\"], na.rm = TRUE)\nmacro_f1        &lt;- mean(cm_caret$byClass[, \"F1\"], na.rm = TRUE)\n\ncat(\"\\nMacro-averaged:\\n\")\n\n\n\nMacro-averaged:\n\n\nShow code\ncat(\"Precision:\", round(macro_precision, 3), \"\\n\")\n\n\nPrecision: 0.675 \n\n\nShow code\ncat(\"Recall:   \", round(macro_recall,    3), \"\\n\")\n\n\nRecall:    0.675 \n\n\nShow code\ncat(\"F1:       \", round(macro_f1,        3), \"\\n\")\n\n\nF1:        0.674 \n\n\nShow code\n############################################################\n# Multi-class AUC (macro-averaged)\n############################################################\nmulti_auc &lt;- multiclass.roc(\n  response  = test_data$Placement_Contract_Completion,\n  predictor = rf_pred_prob\n)\n\ncat(\"\\nMulti-class AUC (macro-averaged):\", round(auc(multi_auc), 3), \"\\n\")\n\n\n\nMulti-class AUC (macro-averaged): 0.847 \n\n\nThe model is very good at ranking probabilities correctly (AUC = 0.847) - if you pick one person from each class at random, the model assigns the highest probability to the correct class in 84.7% of cases.\n\n\nVariable Importance\n\n\nShow code\n############################################################\n# Variable importance\n############################################################\ncat(\"\\nVariable Importance (MeanDecreaseGini):\\n\")\n\n\n\nVariable Importance (MeanDecreaseGini):\n\n\nShow code\nrandomForest::importance(rf_model) %&gt;%\n  as.data.frame() %&gt;%\n  rownames_to_column(\"Predictor\") %&gt;%\n  arrange(desc(MeanDecreaseGini)) %&gt;%\n  head(15) %&gt;%               # top 15\n  print()\n\n\n                Predictor In progress       No       Yes MeanDecreaseAccuracy\n1             Core_Traits    44.61575 88.98770 16.066408             86.06533\n2 Emotional_Understanding    54.07565 73.78543 34.761558             87.00799\n3              Principles    47.78354 81.97185  9.275946             80.24299\n4               Judgement    33.48851 45.19150 13.086894             53.07396\n5    Composite_Risk_Score    30.32766 56.88619  3.774848             57.22316\n6             Uncertainty    22.22296 51.93331 16.833642             52.50164\n  MeanDecreaseGini\n1         41.04164\n2         40.51270\n3         37.21400\n4         31.54092\n5         29.72965\n6         19.30408\n\n\nTop predictors:\n\nCore Traits (41.0) — strongest overall driver\nEmotional Understanding (40.5) - nearly as influential\nPrinciples (37.2)\n\nIn the second model, these are the three features that carry the majority of predictive power. In other words, they are the most important variables for distinguishing work placement contract completion.\n\n\nShow code\nrandomForest::varImpPlot(\n  rf_model,\n  main = \"Random Forest Variable Importance - Placement Contract Completion\",\n  n.var = 6,               # show top 6 predictors. \n  type = 2                 # type = 2 for Gini. \n)"
  },
  {
    "objectID": "psychometrics-candidate-outcomes.html#summary-and-conclusion",
    "href": "psychometrics-candidate-outcomes.html#summary-and-conclusion",
    "title": "Psychometric Risk Assessment: Employment & Placement Outcomes",
    "section": "Summary and Conclusion",
    "text": "Summary and Conclusion\nThis analysis evaluated the predictive validity of the Dynamic Risk Assessment (DRA) in relation to key employment outcomes. Using synthetic data designed to mirror realistic distributions and patterns from the original data set, we conducted descriptive statistics, visual comparisons, ROC curve evaluations, and random forest modelling.\nThe DRA provides valuable insights for recruitment and placement, with its composite risk score and dimensions (particularly Emotional Understanding and Principles) effectively distinguishing between outcome groups in many cases. The analysis supports its use in multi-hurdle screening, where it helps identify low-risk candidates for stable employment pathways. Limitations such as range restriction and the synthetic nature of the data mean results should be viewed as directional rather than definitive. Future work could explore integrating DRA scores with additional data sources (e.g., engagement metrics) to enhance long-term predictions."
  }
]